# Repositorio ETL de Airflow para RBMA Movilidad

## 1. VisiÃ³n General del Proyecto

Este repositorio contiene las implementaciones de los pipelines de **ETL (ExtracciÃ³n, TransformaciÃ³n y Carga)** para el proyecto de **Movilidad Sostenible en la Reserva de la Biosfera del Macizo de Anaga (RBMA)**. La iniciativa, impulsada por el Cabildo de Tenerife en colaboraciÃ³n con la Universidad de La Laguna y con financiaciÃ³n de los fondos Next Generation EU, tiene como objetivo principal gestionar el flujo de mÃ¡s de 1.8 millones de visitantes anuales para mitigar la saturaciÃ³n de vehÃ­culos, reducir el impacto ambiental y mejorar la experiencia de residentes y turistas.

El nÃºcleo tÃ©cnico de este proyecto se basa en la instalaciÃ³n de una red de sensores y cÃ¡maras para monitorizar en tiempo real los patrones de movilidad. Los datos recopilados son procesados a travÃ©s de los pipelines de Airflow definidos en este repositorio. El objetivo final es generar un mapa de movilidad, crear matrices origen-destino y desarrollar algoritmos que permitan a las autoridades tomar decisiones informadas para optimizar el transporte pÃºblico y proponer alternativas sostenibles al vehÃ­culo privado.

Este repositorio se encarga de la orquestaciÃ³n de todos los procesos de datos, desde la ingesta inicial hasta su almacenamiento en un **Data Lake** centralizado, listo para su posterior anÃ¡lisis y visualizaciÃ³n.

## 2. Arquitectura y Estructura del Repositorio

El proyecto estÃ¡ organizado siguiendo una estructura modular y escalable para facilitar el desarrollo, mantenimiento y la reutilizaciÃ³n de cÃ³digo.

```
â””â”€â”€ airflow-rbma/
    â”œâ”€â”€ docs/
    â”‚   â””â”€â”€ release-it_guide.md
    â”œâ”€â”€ lib/
    â”‚   â”œâ”€â”€ params/
    â”‚   â”‚   â””â”€â”€ default_params.py
    â”‚   â”œâ”€â”€ utils/
    â”‚   â”‚   â”œâ”€â”€ dag_tasks/
    â”‚   â”‚   â”‚   â”œâ”€â”€ data_retrieving.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ data_saving.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ datalake.py
    â”‚   â”‚   â”‚   â””â”€â”€ db_multiextract.py
    â”‚   â”‚   â”œâ”€â”€ scripts/
    â”‚   â”‚   â”‚   â”œâ”€â”€ airflow/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ schemas_dags_serializer.py
    â”‚   â”‚   â”‚   â”œâ”€â”€ auto_doc/
    â”‚   â”‚   â”‚   â”‚   â””â”€â”€ generate.py
    â”‚   â”‚   â”‚   â””â”€â”€ local_airflow_manager.py
    â”‚   â”‚   â”œâ”€â”€ basic.py
    â”‚   â”‚   â”œâ”€â”€ date.py
    â”‚   â”‚   â”œâ”€â”€ mail.py
    â”‚   â”‚   â””â”€â”€ params_maker_utils.py
    â”‚   â””â”€â”€ airflow_loggin.py
    â”œâ”€â”€ org_simulator/
    â”‚   â””â”€â”€ bz_0010_sim_extract.py
    â”œâ”€â”€ airflow_global_variables_DEV.json
    â”œâ”€â”€ package-lock.json
    â””â”€â”€ package.json
```

*   **`docs/`**: Contiene toda la documentaciÃ³n relevante del proyecto, incluyendo guÃ­as de despliegue, configuraciÃ³n de herramientas y notas sobre las versiones.
*   **`lib/`**: Es el corazÃ³n del framework ETL. Alberga cÃ³digo reutilizable y mÃ³dulos compartidos que son utilizados por todos los DAGs. Su objetivo es abstraer la lÃ³gica comÃºn y estandarizar el desarrollo.
*   **`org_simulator/`**: Un ejemplo de directorio de "Ã¡mbito" o "esquema". Cada carpeta en la raÃ­z del proyecto (como esta) agrupa DAGs relacionados con una fuente de datos o un propÃ³sito lÃ³gico especÃ­fico. El fichero `bz_0010_sim_extract.py` representa un DAG de ejemplo que utiliza el simulador de datos de cÃ¡maras.
*   **`airflow_global_variables_DEV.json`**: Fichero de configuraciÃ³n que contiene las variables globales de Airflow para el entorno de desarrollo (DEV).
*   **`package.json`**: Define los metadatos del proyecto, las dependencias de Node.js para herramientas de desarrollo y los scripts para automatizar tareas como el versionado o la ejecuciÃ³n de utilidades.

## 3. Componentes Principales del Framework (`lib/`)

La carpeta `lib/` proporciona un conjunto de herramientas robustas para construir pipelines ETL de manera consistente.

####  **Logging Centralizado (`airflow_loggin.py`)**
Proporciona clases personalizadas (`Logger` y `ETLDebugger`) que extienden el sistema de logging de Airflow. Permite registrar mensajes con niveles de detalle configurables y con un formato estandarizado para cada etapa del proceso (ExtracciÃ³n, TransformaciÃ³n, Carga), facilitando la depuraciÃ³n y monitorizaciÃ³n de los DAGs.

#### ğŸï¸ **GestiÃ³n del Data Lake (`lib/utils/dag_tasks/datalake.py`)**
Este mÃ³dulo es fundamental para la interacciÃ³n con el Data Lake.
*   **`multi_saving`**: Una funciÃ³n clave que permite guardar un DataFrame en mÃºltiples destinos de forma simultÃ¡nea (ej. base de datos PostgreSQL, ficheros en un recurso compartido Samba, o en local). Su comportamiento se configura mediante variables de Airflow, permitiendo flexibilidad entre entornos.
*   **`DatalakeDSTView`**: Una clase que facilita la creaciÃ³n y gestiÃ³n de vistas en la base de datos del Data Lake, permitiendo crear "espejos" (mirroring) de tablas de origen o vistas personalizadas a partir de ficheros SQL.
*   **`register_update`**: Registra cada actualizaciÃ³n en una tabla de metadatos, llevando un control sobre los cambios en el nÃºmero de registros de cada tabla del Data Lake.

#### ğŸ’½ **AbstracciÃ³n de Datos (`data_retrieving.py` y `data_saving.py`)**
Estos mÃ³dulos abstraen la lÃ³gica para conectarse y operar con distintas fuentes y destinos de datos (PostgreSQL, Oracle, ficheros CSV, Excel, Parquet). Proporcionan funciones simplificadas para que los DAGs no necesiten conocer los detalles de implementaciÃ³n de las conexiones. Incluye validadores para asegurar la integridad de los datos al escribirlos en bases de datos.

#### ğŸ›ï¸ **ConfiguraciÃ³n de DAGs (`lib/params/default_params.py`)**
Centraliza la configuraciÃ³n por defecto para todos los DAGs (`default_args`), como el propietario, el nÃºmero de reintentos, o la fecha de inicio. Esto asegura consistencia y reduce cÃ³digo repetido en la definiciÃ³n de cada DAG.

## 4. Flujo de Trabajo y Herramientas

El repositorio estÃ¡ equipado con herramientas que automatizan y estandarizan el ciclo de vida del desarrollo.

#### âš™ï¸ **GestiÃ³n de la ConfiguraciÃ³n**
El proyecto utiliza **Variables de Airflow** para gestionar la configuraciÃ³n de manera centralizada. El fichero `airflow_global_variables_DEV.json` sirve como plantilla para los distintos entornos. Variables como `DALAKE_DB_CONN` o `DATALAKE_MULTISAVING_OPTIONS` (conexiÃ³nes directas entre el servicio de Airflow y la instancia desplegada de Postgres -datalake- para este proyecto) permiten adaptar el comportamiento de los DAGs sin modificar el cÃ³digo.

#### ğŸ“¦ **Versionado y Despliegue con `release-it`**
Se utiliza `release-it` junto con el estÃ¡ndar de **Conventional Commits** para automatizar el proceso de creaciÃ³n de versiones. Cada vez que se necesita una nueva release, el siguiente comando se encarga de:
1.  Determinar la siguiente versiÃ³n semÃ¡ntica (major, minor, patch) basÃ¡ndose en los mensajes de commit.
2.  Generar y/o actualizar el fichero `CHANGELOG.md`.
3.  Crear un tag de Git y un commit de versiÃ³n.

```bash
npm run release
```

#### âœï¸ **EstÃ¡ndar de Commits con `commitlint`**
Para garantizar un historial de commits limpio y legible, se utiliza `commitlint`. Este fuerza a que cada mensaje de commit siga la especificaciÃ³n de Conventional Commits (ej. `feat:`, `fix:`, `chore:`, etc.). Esto es crucial para el funcionamiento del versionado automÃ¡tico.

#### ğŸš€ **Scripts de Desarrollo**
`package.json` incluye scripts Ãºtiles para el desarrollo diario:
*   **`npm run airflow -- refresh <nombre_del_esquema>`**: Ejecuta el script `local_airflow_manager.py`, que permite "refrescar" o reserializar los DAGs de un esquema especÃ­fico directamente en el scheduler de Airflow sin necesidad de reiniciarlo. Esto acelera enormemente el ciclo de desarrollo.
*   **`npm run auto-doc`**: Un script avanzado que utiliza la **API de Gemini de Google** para analizar los commits entre dos tags de Git y generar automÃ¡ticamente una propuesta de texto para la nueva secciÃ³n del `CHANGELOG.md`.

## 5. CÃ³mo Empezar

1.  **Clonar el Repositorio**:
    ```bash
    git clone https://github.com/RBMA-Movilidad-ULL-CabildoTNF/airflow-rbma.git
    cd airflow-rbma
    ```

2.  **Instalar Dependencias de Node.js**:
    Estas dependencias son para las herramientas de desarrollo.
    ```bash
    npm install
    ```

3.  **Configurar el Entorno de Airflow**:
    *   AsegÃºrate de que tu entorno de Airflow tiene acceso a las librerÃ­as Python definidas en la carpeta `lib/`.
    *   Importa las variables desde `airflow_global_variables_DEV.json` en la secciÃ³n `Admin -> Variables` de la UI de Airflow y ajÃºstalas segÃºn tu entorno.

4.  **Desarrollar un Nuevo DAG**:
    *   Crea una nueva carpeta en la raÃ­z para agrupar tus DAGs si pertenecen a un nuevo Ã¡mbito.
    *   Dentro de tu DAG, importa los mÃ³dulos necesarios de la carpeta `lib/` para reutilizar la lÃ³gica de logging, extracciÃ³n y carga. El DAG en `org_simulator/bz_0010_sim_extract.py` es un excelente punto de partida.

## 6. Licencia

Este proyecto estÃ¡ distribuido bajo la licencia **ISC**. Para mÃ¡s detalles, consulta el fichero `package.json`.
